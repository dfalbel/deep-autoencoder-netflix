---
title: "Deep Autoencoders for Collaborative Filtering"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```

In this tutorial we will reproduce using Keras the results from [*Training Deep AutoEncoders for Collaborative Filtering*](https://arxiv.org/abs/1708.01715). Kuchaiev et al. proposes a deep autoencoder model
for the rating prediction task in recommender systems which significantly outperforms previous 
state-of-the-art models on a time-split Netflix data set. The original code (in pytorch) is available [here](https://github.com/NVIDIA/DeepRecommender).

Many popular matrix factorization techniques can be thought of as a form of dimensionality reduction. It is, therefore, natural to adapt deep autoencoders for this task as well. There are many other approaches to the recommendation problem using Deep Learning. In particular there is [U-AutoRec](http://users.cecs.anu.edu.au/~u5098633/papers/www15.pdf) that uses Autoencoders for collaborative filtering. 

There are also many non deep learning approaches to this problem many of them implemented in [rsparse package](https://github.com/dselivanov/rsparse).

In this tutorial we will demonstrate some advanced features of Keras such as creating custom layers, loss functions and metrics. We will also use the [tfdatasets](https://github.com/rstudio/tfdatasets) package to make data input faster.

## Dataset

We will use the Netflix Dataset in our experiments. This data set was released when Netflix held the
Netflix Prize open competition for the best algorithm to predict user ratings for films. This is a long session of data manipulation, if you want to focus on the Keras model you can download the processed data from [here]() and go directly to the [model definition session]().

### Downloading and reading

We can download the dataset from [Archive.org](https://archive.org/download/nf_prize_dataset.tar) or directly from R with the code below.

```{r, eval = FALSE}
# This will take some time to downlaod since it's a 660MB file.
download.file(
  url = "https://archive.org/download/nf_prize_dataset.tar/nf_prize_dataset.tar.gz", 
  destfile = "data-raw/nf_prize_dataset.tar.gz"
  )
```

After downloading we will extract files from the compressed archive. We can do it from R with:

```{r, eval = FALSE}
untar("nf_prize_dataset.tar.gz", exdir = "data-raw")
untar("data-raw/download/training_set.tar", exdir = "data-raw")
```

We first extracted all files from the `nf_prize_dataset.tar.gz` to the `data-raw` directory. Then, one of the files is another compressed tar that contains the training set. We again extracted it to the `data-raw` folder.

This dataset comes in an unusual format. There's one file per movie, and each file is a csv with users ids, ratings and date of rating for that particular movie. The first line of the file keeps the movie_id. We will now load the data set into R, so first we will define a function to parse each file, and then loop throught all files using `purrr`.

```{r}
library(tidyverse)
parse_movie <- function(file) {
  movie_id <- read_lines(file, n_max = 1) %>% 
    parse_number()
  df <- read_csv(
    file, 
    skip = 1, 
    col_names = c("user_id", "rating", "date"), 
    col_types = cols(
      user_id = col_integer(),
      rating = col_integer(),
      date = col_date(format = "")
    )
    )
  df$movie_id <- movie_id
  df
}
```

Now let's loop throught all files:

```{r}
files <- dir("data-raw/training_set/", full.names = TRUE)
df <- map_df(files, parse_movie)
head(df)
```

### Preprocessing

We will now split our dataset into training/testing intervals based on time. Training intervals will contain rating that were made earlier than the ones in testing interval. We then randomly splitted the testing interval into test and validation sets. After splitting the data we will convert it to an sparse matrix to make it easier to use in our models. We will also remove from the test interval users that were not found in the training set.

The time splits that were used can be seen in the table below:

|Dataset Name |Training Start Date |Training End Date |Testing Month |
|:------------|:-------------------|:-----------------|:-------------|
|3 months     |2005-09-01          |2005-11-30        |2005-12       |

The paper defines other dataset names with diferent time intervals.

We will now define two functions that will help us preprocess the data. The `split_netflix_data` function takes the full dataset and the training data interval and will split the dataset into the parts we described above. It will also show us some stats about the dataset like the number of unique users.

```{r}
split_netflix_data <- function(df, min_date, max_date) {
  
  x <- df %>%
    filter(between(date, min_date, max_date))
  
  y <- df %>%
    filter(
      between(date, max_date + days(1), max_date + months(1) - days(1)),
      user_id %in% unique(x$user_id), 
      movie_id %in% unique(x$movie_id)
    )
  
  # split test and validation randomly
  ind_validation <- sample.int(nrow(y), nrow(y)/2)
  
  y_test <- y[-ind_validation,]
  y_val <- y[ind_validation,]
  
  cat("Dataset Info \n")
  cat("min_date = ", as.character(min_date), " max_date = ", as.character(max_date), "\n")
  cat("Train -> #Users ", length(unique(x$uid)), " #Ratings ", nrow(x), "\n")
  cat("Test  -> #Users ", length(unique(y_test$uid)), " #Ratings ", nrow(y_test), "\n")
  cat("Valid -> #Users ", length(unique(y_val$uid)), " #Ratings ", nrow(y_val), "\n")
  
  list(x = x, y_val = y_val, y_test = y_test)
} 
```

The `to_sparse` function will convert the datasets to sparse matrixes. It will take care of movie_ids and user_ids so rows and columns are compatible between the training, testing and validation sets. It will return a nested list containing all datasets. We will be able to acess training data with `netflix_data$train$x` for example.

```{r}
to_sparse <- function(netflix_data) {
  
  x <- netflix_data$x
  y_val <- netflix_data$y_val
  y_test <- netflix_data$y_test
  
  
  uids <- data_frame(
    uid = unique(x$uid),
    user_id = row_number(uid)
  )
  
  mids <- data_frame(
    mid = unique(x$mid),
    movie_id = row_number(mid)
  )
  
  x <- x %>%
    left_join(uids, by = "uid") %>%
    left_join(mids, by = "mid")
  
  y_val <- y_val %>%
    left_join(uids, by = "uid") %>%
    left_join(mids, by = "mid")
  
  y_test <- y_test %>%
    left_join(uids, by = "uid") %>%
    left_join(mids, by = "mid")
  
  x <- as(sparseMatrix(x$user_id, x$movie_id, x = x$rating), "dgTMatrix")
  
  # validation data
  y_val <- as(sparseMatrix(y_val$user_id, y_val$movie_id, x = y_val$rating), "dgTMatrix")
  ind_ratings <- which(rowSums(y_val) > 0)
  
  y_val <- y_val[ind_ratings,]
  x_val <- x[ind_ratings,]
  
  # test data
  y_test <- as(sparseMatrix(y_test$user_id, y_test$movie_id, x = y_test$rating), "dgTMatrix")
  ind_ratings <- which(rowSums(y_test) > 0)
  
  y_test <- y_test[ind_ratings,]
  x_test <- x[ind_ratings,]
  
  list(
    train = list(x = x, y = x),
    val = list(x = x_val, y = y_val),
    test = list(x = x_test, y = y_test)
  )
}
```

Now let's create our dataset and save it to the disk.

```{r}
netflix_3m <- split_netflix_data(df, ymd("2005-09-01"), ymd("2005-11-30"))
netflix_3m <- to_sparse(netflix_3m)
saveRDS(netflix_3m, "data/netflix_3m.rds")
```

We now have our data prepared to run our experiments.

## Model

We will train the same model descried in the paper [*Training Deep AutoEncoders for Collaborative Filtering*](https://arxiv.org/abs/1708.01715). An Autoencoder is a neural network that learns two transformations: the $encoder(x): \!R^n \Rightarrow \!R^d$ and $decoder(z): \!R^d \Rightarrow \!R^n$. The idea is to obtain a $d$-dimensional representation of data such that a loss function between $x$ and $f(x) = decoder(encoder(x))$ is minimized. In Kuchaiev et al  encoder and decoder are fully connected layers computing $l=f(W*x + b)$ where $f$ is an activation function. They found that using activation functions containing non-zero negative parts to very very important to the model performance. Also, Kuchaiev et al uses constrained autoencoders, i.e. encoder and decoder layers are mirrored and shares weights.

The model is trained using the Masked Mean Squared Error. This is almost equivalent to the Mean Squared Error except that it doesn't consider the error for items we don't know the user rating. The loss function is defined as:

$$MMSE = \frac{m_i*(r_i - y_i)^2}{\sum_{i=0}^{i=n}m_i}$$
where $r_i$ is the actual rating, $y_i$ is the predicted rating and $m_i$ is a mask function such that $m_i= 1$ if $r_i \ne 0$ else $m_i = 0$. We used Stochastic Gradient Descent with momentum for optimization. The paper proposes a technique called Dense Re-Feeding which we won't implement in this tutorial.

The figure below represents an autoencoder with two layers.

![](autoencoder.png)

## Implementation

Let's now define our model in Keras. Implementing this model requires using some advanced features of Keras like ccreating custom layers and loss functions that we will in explain in detail.

Since we want a tied autoencoder, ie. we want the encoder weights to be shared with the corresponding decoder weights. In the figure above this can e translated to $W^1_d = (W^1_e)^\intercal$ and $W^2_d = (W^2_e)^\intercal$. In Keras, we can define a custom layer that will receive a dense layer as an argument and return a new one with the weights transposed. Below we defined a new layer called `TiedDenseLayer` that will do this.

A custom layer in Keras is an `R6` class that inherits the `KerasLayer` class. You can learn more [here](https://keras.rstudio.com/articles/custom_layers.html) but in short we implemented four methods.

- `initialize(output_dim, master_layer)`: At initilization we specify wich will be the primary layer that we want to share weights with.
- `build(input_shape)`: This is where we defined our weights. We specified the `W` term to be the transpose `W` from the primary layer. We also created a bias term using the function `add_weights` (this is import so Keras can count the parameters on the model).
- `call(x)`: This is where the layer’s logic lives.
- `compute_output_shape(input_shape)`: We return a list with the output shape. 

```{r}
TiedDenseLayer <- R6::R6Class(
  "TiedDenseLayer",
  inherit = KerasLayer,
  public = list(
    
    primary_layer = NULL,
    W = NULL,
    b = NULL,
    output_dim = NULL,
    
    initialize = function(output_dim, primary_layer) {
      self$primary_layer <- primary_layer
    },
    
    build = function(input_shape) {
      # the weights W are just the transposed weights W of the master layer
      self$W <- k_transpose(self$primary_layer$weights[[1]])
      self$output_dim <- self$W$shape$as_list()[[2]]
      
      # we create a new bias weigths
      self$b <- self$add_weight(
        name = 'bias',
        shape = list(self$output_dim),
        initializer = initializer_constant(0),
        trainable = TRUE
      )
      
    },
    
    call = function(x, mask = NULL) {
      k_dot(x, self$W) + self$b # the same as dense layer
    },
    
    compute_output_shape = function(input_shape) {
      list(input_shape[[1]], self$output_dim)
    }
    
  )
)

# wraper to use the layer in R
layer_tied_dense <- function(object, primary_layer, name = NULL, trainable = TRUE) {
  create_layer(TiedDenseLayer, object, list(
    primary_layer = primary_layer,
    name = name,
    trainable = trainable
  ))
}
```








